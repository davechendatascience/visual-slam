{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Online (Incremental) Gaussian Splatting [Pure PyTorch Edition]\n",
                "\n",
                "This notebook implements an **Online / Incremental** 3D reconstruction pipeline using **Pure PyTorch**.\n",
                "\n",
                "**Updates**:\n",
                "- **Pure PyTorch Rasterizer**: Robust, zero-install, works on any Colab runtime.\n",
                "- **Point Dilation**: Renders 2x2 pixel blocks to make sparse points visible and improve gradients.\n",
                "- **Debug Stats**: Logs render vs ground truth intensity to verify tracking.\n",
                "- **Stability Fix**: Uses `torch.logit` with clamping to prevent NaN losses.\n",
                "- **Dual Depth Support**: Toggle between AI Depth (UniDepth) and Sensor Depth (Kinect).\n",
                "- **Metrics & Viz**: Added PSNR, Rolling Average Loss, and Error Heatmaps.\n",
                "- **Edge Cleaning**: Added Median Blur to sensor depth to reduce ghosting.\n",
                "\n",
                "**Pipeline**:\n",
                "1.  **Process frames sequentially**.\n",
                "2.  **Spawn New Gaussians** using UniDepth or Kinect.\n",
                "3.  **Optimization** using a custom Python rasterizer (with dilation).\n",
                "4.  **Live Visualization**.\n",
                "\n",
                "**Hardware Requirement**: T4 GPU (free tier) or A100 (Pro) on Google Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment\n",
                "!nvidia-smi\n",
                "\n",
                "# Install core libs (No gsplat needed!)\n",
                "!pip install torch torchvision torchaudio tqdm opencv-python matplotlib scipy pandas imageio plyfile plotly\n",
                "\n",
                "# Install UniDepth\n",
                "import os\n",
                "import sys\n",
                "\n",
                "if not os.path.exists(\"UniDepth\"):\n",
                "    !git clone https://github.com/lpiccinelli-eth/UniDepth.git\n",
                "\n",
                "%cd UniDepth\n",
                "!pip install -e .\n",
                "!pip install timm huggingface_hub\n",
                "%cd ..\n",
                "\n",
                "# CRITICAL: Add UniDepth to python path so it can be imported immediately\n",
                "sys.path.append(\"/content/UniDepth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Download TUM Dataset (fr3_office)\n",
                "%cd /content\n",
                "!mkdir -p datasets/tum\n",
                "%cd datasets/tum\n",
                "\n",
                "# Fixed URL using vision.in.tum.de\n",
                "dataset_url = \"https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_long_office_household.tgz\"\n",
                "!wget -O dataset.tgz {dataset_url}\n",
                "!tar -xzf dataset.tgz\n",
                "!mv rgbd_dataset_freiburg3_long_office_household fr3_office\n",
                "%cd /content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Metric Depth (UniDepth)\n",
                "We pre-process the sequence to get clean metric depth maps for initialization.\n",
                "(In a real C++ system this would run live, but for Python Colab we pre-cache it for speed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import os\n",
                "import glob\n",
                "from PIL import Image\n",
                "from tqdm import tqdm\n",
                "from unidepth.models import UniDepthV2\n",
                "\n",
                "def generate_unidepth(dataset_path, output_depth_path):\n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    print(\"Loading UniDepth V2...\")\n",
                "    model = UniDepthV2.from_pretrained(\"lpiccinelli/unidepth-v2-vitl14\").to(device).eval()\n",
                "    \n",
                "    rgb_path = os.path.join(dataset_path, \"rgb\")\n",
                "    if not os.path.exists(output_depth_path):\n",
                "        os.makedirs(output_depth_path)\n",
                "        \n",
                "    image_files = sorted(glob.glob(os.path.join(rgb_path, \"*.png\")))\n",
                "    # Process every nth frame.\n",
                "    image_files = image_files[::1] \n",
                "    \n",
                "    print(f\"Processing {len(image_files)} frames...\")\n",
                "    with torch.no_grad():\n",
                "        for img_path in tqdm(image_files):\n",
                "            pil_img = Image.open(img_path).convert(\"RGB\")\n",
                "            rgb_tensor = torch.from_numpy(np.array(pil_img)).permute(2, 0, 1).unsqueeze(0).to(device)\n",
                "            \n",
                "            predictions = model.infer(rgb_tensor)\n",
                "            depth_map = predictions[\"depth\"].squeeze().cpu().numpy()\n",
                "            \n",
                "            # Save as .npy for full float precision (easier for custom loader)\n",
                "            basename = os.path.basename(img_path).replace(\".png\", \".npy\")\n",
                "            np.save(os.path.join(output_depth_path, basename), depth_map)\n",
                "\n",
                "dataset_root = \"/content/datasets/tum/fr3_office\"\n",
                "depth_root = os.path.join(dataset_root, \"depth_unidepth\")\n",
                "generate_unidepth(dataset_root, depth_root)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Custom TUM Dataset Loader & Pure PyTorch Rasterizer (Dilated)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from scipy.spatial.transform import Rotation\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "# --- Dataset Loader ---\n",
                "def associate_data(root_dir):\n",
                "    # Helper to read TUM format files\n",
                "    def read_file_list(filename):\n",
                "        file = open(filename)\n",
                "        data = file.read()\n",
                "        lines = data.replace(\",\", \" \").replace(\"\\t\", \" \").split(\"\\n\")\n",
                "        list = [[v.strip() for v in line.split(\" \") if v.strip() != \"\"] for line in lines if len(line) > 0 and line[0] != \"#\"]\n",
                "        list = [(float(l[0]), l[1:]) for l in list if len(l) > 1]\n",
                "        return dict(list)\n",
                "\n",
                "    rgb_list = read_file_list(os.path.join(root_dir, \"rgb.txt\"))\n",
                "    gt_list = read_file_list(os.path.join(root_dir, \"groundtruth.txt\"))\n",
                "    def_list = read_file_list(os.path.join(root_dir, \"depth.txt\")) # Load Sensor Depth List\n",
                "    \n",
                "    rgb_timestamps = sorted(rgb_list.keys())\n",
                "    gt_timestamps = sorted(gt_list.keys())\n",
                "    dep_timestamps = sorted(def_list.keys())\n",
                "    \n",
                "    matches = []\n",
                "    offset = 0.0\n",
                "    max_difference = 0.02\n",
                "    \n",
                "    for t in rgb_timestamps:\n",
                "        t_with_offset = t + offset\n",
                "        \n",
                "        # Associate GT Poses\n",
                "        best_gt_t = min(gt_timestamps, key=lambda x: abs(x - t_with_offset))\n",
                "        gt_diff = abs(best_gt_t - t_with_offset)\n",
                "        \n",
                "        # Associate Sensor Depth\n",
                "        best_dep_t = min(dep_timestamps, key=lambda x: abs(x - t_with_offset))\n",
                "        dep_diff = abs(best_dep_t - t_with_offset)\n",
                "        \n",
                "        if gt_diff < max_difference and dep_diff < max_difference:\n",
                "            matches.append((t, best_gt_t, best_dep_t))\n",
                "            \n",
                "    data = []\n",
                "    for t_rgb, t_gt, t_dep in matches:\n",
                "        rgb_file = rgb_list[t_rgb][0]\n",
                "        \n",
                "        # AI Depth Path\n",
                "        unidepth_file_name = os.path.basename(rgb_file).replace(\".png\", \".npy\")\n",
                "        unidepth_path = os.path.join(root_dir, \"depth_unidepth\", unidepth_file_name)\n",
                "        \n",
                "        # Sensor Depth Path\n",
                "        sensor_depth_file = def_list[t_dep][0]\n",
                "        sensor_depth_path = os.path.join(root_dir, sensor_depth_file)\n",
                "        \n",
                "        gt_data = gt_list[t_gt]\n",
                "        tx, ty, tz = float(gt_data[0]), float(gt_data[1]), float(gt_data[2])\n",
                "        qx, qy, qz, qw = float(gt_data[3]), float(gt_data[4]), float(gt_data[5]), float(gt_data[6])\n",
                "        \n",
                "        rot = Rotation.from_quat([qx, qy, qz, qw]).as_matrix()\n",
                "        c2w = np.eye(4)\n",
                "        c2w[:3, :3] = rot\n",
                "        c2w[:3, 3] = [tx, ty, tz]\n",
                "        \n",
                "        data.append({\n",
                "            \"rgb_path\": os.path.join(root_dir, rgb_file),\n",
                "            \"unidepth_path\": unidepth_path,\n",
                "            \"sensor_depth_path\": sensor_depth_path,\n",
                "            \"c2w\": c2w,\n",
                "            \"timestamp\": t_rgb\n",
                "        })\n",
                "    return data\n",
                "\n",
                "dataset_data = associate_data(dataset_root)\n",
                "print(f\"Associated {len(dataset_data)} frames (RGB + GT + SensorDepth).\")\n",
                "\n",
                "# --- Pure PyTorch Rasterizer (with Dilation) ---\n",
                "def pure_pytorch_rasterization(means, colors, opacities, scales, quats, viewmat, K, height, width):\n",
                "    # 1. World -> Camera\n",
                "    R = viewmat[:3, :3]\n",
                "    t = viewmat[:3, 3]\n",
                "    \n",
                "    means_c = (R @ means.T).T + t\n",
                "    \n",
                "    # Debug info\n",
                "    # if means.shape[0] > 0:\n",
                "    #    print(f\"Z-Range: {means_c[:,2].min():.2f} - {means_c[:,2].max():.2f}\")\n",
                "    \n",
                "    mask = means_c[:, 2] > 0.1 # Near plane\n",
                "    \n",
                "    points = means_c[mask]\n",
                "    colors = colors[mask]\n",
                "    \n",
                "    if points.shape[0] == 0:\n",
                "        return torch.zeros((height, width, 3), device=means.device)\n",
                "    \n",
                "    # 2. Project to Screen (Perspective)\n",
                "    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]\n",
                "    z = points[:, 2]\n",
                "    x = points[:, 0] * fx / z + cx\n",
                "    y = points[:, 1] * fy / z + cy\n",
                "    \n",
                "    # Screen bounds check\n",
                "    mask_screen = (x >= -20) & (x < width + 20) & (y >= -20) & (y < height + 20)\n",
                "    \n",
                "    points = points[mask_screen]\n",
                "    colors = colors[mask_screen]\n",
                "    x = x[mask_screen]\n",
                "    y = y[mask_screen]\n",
                "    z = z[mask_screen]\n",
                "    \n",
                "    if points.shape[0] == 0:\n",
                "        return torch.zeros((height, width, 3), device=means.device)\n",
                "\n",
                "    # 3. Splatting with 2x2 Dilation\n",
                "    # Sort back-to-front\n",
                "    sorted_indices = torch.argsort(z, descending=True)\n",
                "    x = x[sorted_indices]\n",
                "    y = y[sorted_indices]\n",
                "    colors = colors[sorted_indices]\n",
                "    \n",
                "    # Create canvas\n",
                "    canvas = torch.zeros((height, width, 3), device=means.device)\n",
                "    \n",
                "    # Quantize to pixels (Top-Left corner)\n",
                "    ix_base = x.long()\n",
                "    iy_base = y.long()\n",
                "    \n",
                "    # EXPAND indices to 2x2 block (Dilation)\n",
                "    # Offsets: (0,0), (1,0), (0,1), (1,1)\n",
                "    off_x = torch.tensor([0, 1, 0, 1], device=means.device)\n",
                "    off_y = torch.tensor([0, 0, 1, 1], device=means.device)\n",
                "    \n",
                "    # Repeat data 4 times\n",
                "    ix = ix_base.unsqueeze(1) + off_x.unsqueeze(0) # [N, 4]\n",
                "    iy = iy_base.unsqueeze(1) + off_y.unsqueeze(0) # [N, 4]\n",
                "    colors_rep = colors.unsqueeze(1).expand(-1, 4, -1) # [N, 4, 3]\n",
                "    \n",
                "    # Flatten\n",
                "    ix = ix.view(-1)\n",
                "    iy = iy.view(-1)\n",
                "    colors_rep = colors_rep.reshape(-1, 3)\n",
                "    \n",
                "    # Valid check after expansion\n",
                "    valid = (ix >= 0) & (ix < width) & (iy >= 0) & (iy < height)\n",
                "    ix = ix[valid]\n",
                "    iy = iy[valid]\n",
                "    colors_rep = colors_rep[valid]\n",
                "    \n",
                "    # Paint\n",
                "    indices = iy * width + ix\n",
                "    canvas_flat = canvas.reshape(-1, 3)\n",
                "    canvas_flat.index_copy_(0, indices, colors_rep)\n",
                "    canvas = canvas_flat.reshape(height, width, 3)\n",
                "    \n",
                "    return canvas\n",
                "\n",
                "# --- Gaussian Model ---\n",
                "class SimpleGaussianModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.means = nn.Parameter(torch.empty(0, 3, device=\"cuda\"))\n",
                "        self.scales = nn.Parameter(torch.empty(0, 3, device=\"cuda\"))\n",
                "        self.quats = nn.Parameter(torch.empty(0, 4, device=\"cuda\"))\n",
                "        self.opacities = nn.Parameter(torch.empty(0, 1, device=\"cuda\"))\n",
                "        self.colors = nn.Parameter(torch.empty(0, 3, device=\"cuda\"))\n",
                "        \n",
                "    def add_gaussians(self, new_means, new_colors):\n",
                "        with torch.no_grad():\n",
                "            N_new = new_means.shape[0]\n",
                "            if N_new == 0: return\n",
                "            \n",
                "            new_scales = torch.ones(N_new, 3, device=\"cuda\") * -5.0 \n",
                "            new_quats = torch.zeros(N_new, 4, device=\"cuda\"); new_quats[:, 0] = 1.0\n",
                "            new_opacities = torch.zeros(N_new, 1, device=\"cuda\")\n",
                "            \n",
                "            # STABLE INIT: Clamp colors to avoid log(0) -> NaN\n",
                "            new_colors = torch.clamp(new_colors, 0.01, 0.99)\n",
                "            new_colors = torch.logit(new_colors) # Stable inverse sigmoid\n",
                "            \n",
                "            self.means = nn.Parameter(torch.cat([self.means, new_means]))\n",
                "            self.scales = nn.Parameter(torch.cat([self.scales, new_scales]))\n",
                "            self.quats = nn.Parameter(torch.cat([self.quats, new_quats]))\n",
                "            self.opacities = nn.Parameter(torch.cat([self.opacities, new_opacities]))\n",
                "            self.colors = nn.Parameter(torch.cat([self.colors, new_colors]))\n",
                "\n",
                "    def forward(self, viewmat, K, height, width):\n",
                "        render_colors = torch.sigmoid(self.colors)\n",
                "        opacities = torch.sigmoid(self.opacities)\n",
                "        \n",
                "        rgb = pure_pytorch_rasterization(\n",
                "            self.means,\n",
                "            render_colors,\n",
                "            opacities,\n",
                "            torch.exp(self.scales),\n",
                "            self.quats,\n",
                "            viewmat, \n",
                "            K,\n",
                "            height, width\n",
                "        )\n",
                "        return rgb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Online Reconstruction (The Main Loop)\n",
                "We iterate through frames, adding new Gaussians (Keyframing) and optimizing immediately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import display, clear_output\n",
                "\n",
                "# Setup\n",
                "model = SimpleGaussianModel().cuda()\n",
                "\n",
                "# Optimizer config\n",
                "lr_means = 0.00016\n",
                "lr_colors = 0.0025\n",
                "\n",
                "optimizer = torch.optim.Adam([\n",
                "    {'params': [model.means], 'lr': lr_means},\n",
                "    {'params': [model.colors], 'lr': lr_colors},\n",
                "], lr=0.0)\n",
                "\n",
                "# Camera Intrinsics\n",
                "H, W = 480, 640\n",
                "fx, fy, cx, cy = 535.4, 539.2, 320.1, 247.6\n",
                "K_mat = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n",
                "K_torch = torch.tensor(K_mat, device=\"cuda\", dtype=torch.float32)\n",
                "\n",
                "# Params\n",
                "DEPTH_MODE = \"sensor\" # Options: \"unidepth\" (AI) or \"sensor\" (Kinect)\n",
                "KEYFRAME_EVERY = 5 # Add new Gaussians every X frames\n",
                "ITERS_PER_FRAME = 5 # Optimize X steps per frame\n",
                "VIZ_EVERY = 10 # Update plot every X frames\n",
                "\n",
                "\n",
                "def spawn_gaussians_from_frame(frame_data, mode=\"sensor\", subsample=4):\n",
                "    # Load Image\n",
                "    img = np.array(Image.open(frame_data['rgb_path']).convert(\"RGB\")) / 255.0\n",
                "    \n",
                "    # Load Depth based on mode\n",
                "    if mode == \"unidepth\":\n",
                "        # Metric prediction (float32)\n",
                "        depth = np.load(frame_data['unidepth_path'])\n",
                "    else:\n",
                "        # Sensor depth (PNG uint16, scale=5000) from Kinect\n",
                "        depth_png = np.array(Image.open(frame_data['sensor_depth_path']))\n",
                "        \n",
                "        # FIX: Median Blur to remove \"flying pixels\" and ghost edges common in Kinect data\n",
                "        # This fixes the \"duplicated cup\" artifact\n",
                "        depth_png = cv2.medianBlur(depth_png, 5) \n",
                "        \n",
                "        depth = depth_png.astype(np.float32) / 5000.0\n",
                "        # Fix missing values (0) -> make them invalid for masking\n",
                "        depth[depth == 0] = -1.0\n",
                "    \n",
                "    # Backproject\n",
                "    ys, xs = np.indices((H, W))\n",
                "    \n",
                "    # Subsample\n",
                "    ys, xs = ys[::subsample, ::subsample], xs[::subsample, ::subsample]\n",
                "    z = depth[::subsample, ::subsample]\n",
                "    img_small = img[::subsample, ::subsample]\n",
                "    \n",
                "    x = (xs - cx) * z / fx\n",
                "    y = (ys - cy) * z / fy\n",
                "    xyz_cam = np.stack([x, y, z], axis=-1)\n",
                "    \n",
                "    # Transform to World\n",
                "    c2w = frame_data['c2w']\n",
                "    xyz_world = (c2w[:3, :3] @ xyz_cam.reshape(-1, 3).T).T + c2w[:3, 3]\n",
                "    colors = img_small.reshape(-1, 3)\n",
                "    \n",
                "    # Filter invalid (Close, Far, or Missing)\n",
                "    mask = (z.reshape(-1) > 0.1) & (z.reshape(-1) < 8.0)\n",
                "    \n",
                "    return torch.tensor(xyz_world[mask], dtype=torch.float32, device=\"cuda\"), \\\n",
                "           torch.tensor(colors[mask], dtype=torch.float32, device=\"cuda\")\n",
                "\n",
                "\n",
                "# --- Online Loop ---\n",
                "limit_frames = 300 # Process first 300 frames for demo speed\n",
                "process_data = dataset_data[:limit_frames]\n",
                "\n",
                "# Metrics\n",
                "loss_history = []\n",
                "psnr_history = []\n",
                "\n",
                "def get_psnr(pred, gt):\n",
                "    mse = torch.mean((pred - gt) ** 2)\n",
                "    return -10 * torch.log10(mse)\n",
                "\n",
                "print(f\"Starting Online Reconstruction with DEPTH_MODE='{DEPTH_MODE}'...\")\n",
                "for i, frame in enumerate(process_data):\n",
                "    \n",
                "    # 1. Spawn New Gaussians (Keyframing)\n",
                "    if i % KEYFRAME_EVERY == 0:\n",
                "        new_means, new_colors = spawn_gaussians_from_frame(frame, mode=DEPTH_MODE, subsample=8)\n",
                "        model.add_gaussians(new_means, new_colors)\n",
                "        \n",
                "        # Re-add parameters to optimizer (since we changed shape)\n",
                "        optimizer = torch.optim.Adam([\n",
                "            {'params': [model.means], 'lr': lr_means},\n",
                "            {'params': [model.colors], 'lr': lr_colors},\n",
                "        ], lr=0.0)\n",
                "    \n",
                "    # 2. Prepare Data for Optimization\n",
                "    gt_rgb = torch.tensor(np.array(Image.open(frame['rgb_path']).convert(\"RGB\")) / 255.0, dtype=torch.float32, device=\"cuda\")\n",
                "    gt_c2w = torch.tensor(frame['c2w'], dtype=torch.float32, device=\"cuda\")\n",
                "    w2c = torch.inverse(gt_c2w)\n",
                "    \n",
                "    # 3. Incremental Optimization\n",
                "    for _ in range(ITERS_PER_FRAME):\n",
                "        render_rgb = model(w2c, K_torch, H, W)\n",
                "        \n",
                "        loss = torch.abs(render_rgb - gt_rgb).mean()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "    \n",
                "    # Track Metrics\n",
                "    with torch.no_grad():\n",
                "        curr_psnr = get_psnr(render_rgb, gt_rgb).item()\n",
                "        loss_history.append(loss.item())\n",
                "        psnr_history.append(curr_psnr)\n",
                "        avg_loss = np.mean(loss_history[-20:]) # Mean of last 20 frames\n",
                "        avg_psnr = np.mean(psnr_history[-20:])\n",
                "\n",
                "    # 4. Live Visualization\n",
                "    if i % VIZ_EVERY == 0:\n",
                "        render_np = render_rgb.detach().cpu().numpy()\n",
                "        gt_np = gt_rgb.detach().cpu().numpy()\n",
                "        error_np = np.abs(render_np - gt_np).mean(axis=2) # Error Map\n",
                "        \n",
                "        # Print stats to verify valid rendering\n",
                "        print(f\"Frame {i}/{len(process_data)} | Loss: {loss.item():.4f} | Avg Loss (20): {avg_loss:.4f} | PSNR: {curr_psnr:.2f} | Avg PSNR (20): {avg_psnr:.2f} | Points: {model.means.shape[0]}\")\n",
                "\n",
                "        clear_output(wait=True)\n",
                "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
                "        ax[0].imshow(render_np); ax[0].set_title(f\"Render (PSNR: {curr_psnr:.2f})\")\n",
                "        ax[1].imshow(gt_np); ax[1].set_title(\"Ground Truth\")\n",
                "        im = ax[2].imshow(error_np, cmap='jet', vmin=0, vmax=0.5); ax[2].set_title(f\"Error Residual (Avg: {avg_loss:.4f})\")\n",
                "        plt.colorbar(im, ax=ax[2])\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Final Map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from plyfile import PlyData, PlyElement\n",
                "\n",
                "means = model.means.detach().cpu().numpy()\n",
                "colors = torch.sigmoid(model.colors).detach().cpu().numpy()\n",
                "\n",
                "# Create structured array\n",
                "vertex = np.array([tuple(np.concatenate([means[i], colors[i]*255])) for i in range(len(means))],\n",
                "                  dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n",
                "\n",
                "el = PlyElement.describe(vertex, 'vertex')\n",
                "PlyData([el]).write('online_tum_reconstruction.ply')\n",
                "print(\"Saved online_tum_reconstruction.ply\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Interactive Visualization (Plotly)\n",
                "Visualize the resulting Point Cloud directly in the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "import numpy as np\n",
                "from plyfile import PlyData\n",
                "\n",
                "def visualize_ply(ply_path, subsample=10):\n",
                "    print(\"Loading PLY...\")\n",
                "    plydata = PlyData.read(ply_path)\n",
                "    vertex = plydata['vertex']\n",
                "    \n",
                "    # Subsample to avoid crashing browser (e.g., take every 50th point)\n",
                "    x = vertex['x'][::subsample]\n",
                "    y = vertex['y'][::subsample]\n",
                "    z = vertex['z'][::subsample]\n",
                "    r = vertex['red'][::subsample]\n",
                "    g = vertex['green'][::subsample]\n",
                "    b = vertex['blue'][::subsample]\n",
                "    \n",
                "    colors = np.stack([r, g, b], axis=-1) / 255.0\n",
                "    \n",
                "    # Outlier Removal (Keep 5%-95% range)\n",
                "    # If outliers exist (e.g., depth=0 or depth=Inf), they distort the plot scale.\n",
                "    def filter_outliers(arr):\n",
                "        q5, q95 = np.percentile(arr, 5), np.percentile(arr, 95)\n",
                "        return (arr >= q5) & (arr <= q95)\n",
                "        \n",
                "    mask = filter_outliers(x) & filter_outliers(y) & filter_outliers(z)\n",
                "    x, y, z = x[mask], y[mask], z[mask]\n",
                "    colors = colors[mask]\n",
                "    \n",
                "    print(f\"Visualizing {len(x)} points (Subsample={subsample}, Outliers Removed)...\")\n",
                "    \n",
                "    fig = go.Figure(data=[go.Scatter3d(\n",
                "        x=x, y=z, z=y, # Swapped Y/Z for typical rendering orientation\n",
                "        mode='markers',\n",
                "        marker=dict(\n",
                "            size=2,\n",
                "            color=colors,\n",
                "            opacity=0.8\n",
                "        )\n",
                "    )])\n",
                "    \n",
                "    fig.update_layout(\n",
                "        scene=dict(\n",
                "            aspectmode='data', # Ensures 1:1:1 scale based on data range\n",
                "            camera=dict(eye=dict(x=-1.5, y=-1.5, z=0.5))\n",
                "        ),\n",
                "        margin=dict(l=0, r=0, b=0, t=0)\n",
                "    )\n",
                "    fig.show()\n",
                "\n",
                "visualize_ply('online_tum_reconstruction.ply', subsample=50)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}